# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ“Š RÃ¨gles d'Alertes Prometheus
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

groups:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ðŸ–¥ï¸ Alertes Infrastructure
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: infrastructure
    interval: 30s
    rules:
      # Instance down
      - alert: InstanceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Instance {{ $labels.instance }} est DOWN"
          description: "L'instance {{ $labels.instance }} du job {{ $labels.job }} ne rÃ©pond plus depuis plus d'1 minute."
      
      # CPU Ã©levÃ©
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CPU Ã©levÃ© sur {{ $labels.instance }}"
          description: "L'utilisation CPU est de {{ $value | humanize }}% sur {{ $labels.instance }}."
      
      # CPU critique
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CPU CRITIQUE sur {{ $labels.instance }}"
          description: "L'utilisation CPU est de {{ $value | humanize }}% sur {{ $labels.instance }}. Action immÃ©diate requise."
      
      # MÃ©moire Ã©levÃ©e
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MÃ©moire Ã©levÃ©e sur {{ $labels.instance }}"
          description: "L'utilisation mÃ©moire est de {{ $value | humanize }}% sur {{ $labels.instance }}."
      
      # MÃ©moire critique
      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "MÃ©moire CRITIQUE sur {{ $labels.instance }}"
          description: "L'utilisation mÃ©moire est de {{ $value | humanize }}% sur {{ $labels.instance }}. Risque d'OOM."
      
      # Disque plein
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Espace disque faible sur {{ $labels.instance }}"
          description: "Il reste seulement {{ $value | humanize }}% d'espace disque sur {{ $labels.device }} ({{ $labels.mountpoint }})."
      
      # Disque critique
      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Espace disque CRITIQUE sur {{ $labels.instance }}"
          description: "Il reste seulement {{ $value | humanize }}% d'espace disque sur {{ $labels.device }}. Action urgente."

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ðŸ³ Alertes Docker / Conteneurs
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: containers
    interval: 30s
    rules:
      # Conteneur down
      - alert: ContainerDown
        expr: absent(container_last_seen{name!=""})
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Conteneur {{ $labels.name }} est DOWN"
          description: "Le conteneur {{ $labels.name }} ne rÃ©pond plus."
      
      # Conteneur redÃ©marrÃ©
      - alert: ContainerRestarted
        expr: rate(container_last_seen[5m]) > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Conteneur {{ $labels.name }} redÃ©marrÃ©"
          description: "Le conteneur {{ $labels.name }} a redÃ©marrÃ© rÃ©cemment."
      
      # CPU conteneur Ã©levÃ©
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{name!=""}[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CPU Ã©levÃ© pour {{ $labels.name }}"
          description: "Le conteneur {{ $labels.name }} utilise {{ $value | humanize }}% CPU."

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ðŸ§  Alertes Services IA
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: ai_services
    interval: 30s
    rules:
      # Ollama down
      - alert: OllamaDown
        expr: up{job="oceanphenix-core", instance=~"ollama.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ollama LLM Engine est DOWN"
          description: "Le service Ollama ne rÃ©pond plus. Les requÃªtes IA vont Ã©chouer."
      
      # Qdrant down
      - alert: QdrantDown
        expr: up{job="oceanphenix-core", instance=~"qdrant.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Qdrant Vector DB est DOWN"
          description: "La base vectorielle Qdrant ne rÃ©pond plus. Le RAG est indisponible."
      
      # API Backend down
      - alert: BackendAPIDown
        expr: up{job="oceanphenix-core", instance=~"api.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "API Backend est DOWN"
          description: "L'API FastAPI backend ne rÃ©pond plus."

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ðŸ’¾ Alertes Stockage
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: storage
    interval: 1m
    rules:
      # MinIO down
      - alert: MinIODown
        expr: up{job="oceanphenix-core", instance=~"minio.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "MinIO S3 Storage est DOWN"
          description: "Le stockage S3 MinIO ne rÃ©pond plus. Les uploads/downloads vont Ã©chouer."

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ðŸ”„ Alertes Proxy & RÃ©seau
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: proxy
    interval: 30s
    rules:
      # Caddy down
      - alert: CaddyProxyDown
        expr: up{job="oceanphenix-core", instance=~"caddy.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Caddy Reverse Proxy est DOWN"
          description: "Le reverse proxy Caddy ne rÃ©pond plus. Tous les services externes sont inaccessibles."
      
      # Taux d'erreur HTTP Ã©levÃ©
      - alert: HighHTTPErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Taux d'erreur HTTP Ã©levÃ©"
          description: "Le taux d'erreur HTTP 5xx est de {{ $value | humanizePercentage }}."

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ðŸ“ˆ Alertes Monitoring
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: monitoring
    interval: 1m
    rules:
      # Prometheus down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus est DOWN"
          description: "Le systÃ¨me de monitoring Prometheus ne rÃ©pond plus."
      
      # Grafana down
      - alert: GrafanaDown
        expr: up{instance=~"grafana.*"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Grafana Dashboard est DOWN"
          description: "L'interface Grafana ne rÃ©pond plus."
